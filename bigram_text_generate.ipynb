{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import gutenberg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making language model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "import string\n",
    "from nltk import SimpleGoodTuringProbDist\n",
    "from math import log\n",
    "import random\n",
    "from nltk.corpus import twitter_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sudhanshu\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: DeprecationWarning: 'U' mode is deprecated\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Chef salad\n",
      "['Chef', 'salad', 'is', 'calling', 'my', 'name', 'Im', 'so', 'hungry', 'Ngapain', 'lo', 'online', 'sih', 'nif', 'Iyaaaa', 'gituuu', 'oooooh', 'gituuuuu', 'haha', 'Im', 'on', 'my', 'iPod', 'and', 'Im', 'on', 'the', 'tumblr', 'app', 'and']\n",
      "['Chef', 'salad', 'is', 'calling', 'my', 'name', 'Im', 'so', 'hungry', 'Ngapain', 'lo', 'online', 'sih', 'nif', 'Iyaaaa', 'gituuu', 'oooooh', 'gituuuuu', 'haha', 'Im']\n"
     ]
    }
   ],
   "source": [
    " # rU means \"read\", and handles line endings\n",
    "file= open(\"sample.txt\", \"rU\")\n",
    "raw= file.read()\n",
    "print(raw[:11])\n",
    "tokens= nltk.word_tokenize(raw)\n",
    "#sample = nltk.corpus.reader.plaintext.PlaintextCorpusReader(\".\", \"sample.txt\")\n",
    "#test = nltk.corpus.reader.plaintext.PlaintextCorpusReader(\".\", \"test_data.txt\")\n",
    "print(tokens[:30])\n",
    "text= nltk.Text(tokens)\n",
    "print(text[:20])\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[', 'Emma', 'by', 'Jane', 'Austen', '1816', ']', 'VOLUME', 'I', 'CHAPTER', 'I', 'Emma', 'Woodhouse', ',', 'handsome', ',', 'clever', ',', 'and', 'rich', ',', 'with', 'a', 'comfortable', 'home', 'and', 'happy', 'disposition', ',', 'seemed']\n",
      "['[', 'Emma', 'by', 'Jane', 'Austen', '1816', ']', 'VOLUME', 'I', 'CHAPTER', 'I', 'Emma', 'Woodhouse', ',', 'handsome', ',', 'clever', ',', 'and', 'rich']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2621613"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens= nltk.corpus.gutenberg.words()\n",
    "#sample = nltk.corpus.reader.plaintext.PlaintextCorpusReader(\".\", \"sample.txt\")\n",
    "#test = nltk.corpus.reader.plaintext.PlaintextCorpusReader(\".\", \"test_data.txt\")\n",
    "print(tokens[:30])\n",
    "text= nltk.Text(tokens)\n",
    "print(text[:20])\n",
    "len(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unigram perplexity with add 1 smoothing\\ Laplacian smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopwords= set(nltk.corpus.stopwords.words('english'))\n",
    "#stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['knob',\n",
       " 'really',\n",
       " 'undoing',\n",
       " ',',\n",
       " 'turning',\n",
       " '--',\n",
       " 'so',\n",
       " 'now',\n",
       " 'finally',\n",
       " ',',\n",
       " 'Good',\n",
       " '-',\n",
       " 'bye',\n",
       " '--',\n",
       " 'and',\n",
       " 'hail',\n",
       " '!',\n",
       " 'my',\n",
       " 'Fancy',\n",
       " '.']"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[-20:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42334"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram= [w.lower() for w in tokens if w not in stopwords]\n",
    "unigram= list(set(unigram))\n",
    "len(unigram)\n",
    "#print(unigram[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spl= int(95*len(unigram)/100)\n",
    "train_corpus= unigram[:spl]\n",
    "test_corpus= unigram[spl:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "freq_1gram= nltk.FreqDist(train_corpus)\n",
    "len_train= len(train_corpus)\n",
    "vocab= len(set(train_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unigram_prob_with_add1smoothing(word):\n",
    "    return (freq_1gram[word]+1)/(len_train+vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def entropy(n, text):\n",
    "        e = 0.0\n",
    "        text = [\"<s>\"] + text + [\"</s>\"]\n",
    "        for i in range(n - 1, len(text)):\n",
    "            context = text[i - n + 1:i]\n",
    "            token = text[i]\n",
    "            #print(str(context)+\"    \"+token)\n",
    "            e += logprob(token, context)\n",
    "        return e / float(len(text) - (n - 1))\n",
    "\n",
    "\n",
    "def logprob(word, context):\n",
    "    if len(context)==0:\n",
    "        p=unigram_prob_with_add1smoothing(word)\n",
    "    elif len(context)==1:\n",
    "        p=bigram_prob_with_add1smoothing(context[0], word)\n",
    "    else:\n",
    "        p=trigram_prob_with_add1smoothing(context[0], context[1], word)\n",
    "    return -p*log(p , 2)\n",
    "\n",
    "\n",
    "def perplexity(n, text):\n",
    "      return pow(2.0, entropy(n, text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.000140437941288"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perplexity(1, test_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0002636496062822"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perplexity(1, train_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigram Perplexity with add 1 smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('<s>', 'gallowes'), ('gallowes', 'therin'), ('therin', 'circumvented'), ('circumvented', 'leuell'), ('leuell', 'himmalehs'), ('himmalehs', 'forsaking'), ('forsaking', 'mitchell'), ('mitchell', 'civil'), ('civil', 'purchases'), ('purchases', 'marcus'), ('marcus', 'purveyed'), ('purveyed', 'maritime'), ('maritime', 'vpspring'), ('vpspring', 'surging'), ('surging', 'palsied'), ('palsied', 'disinterested'), ('disinterested', 'arrowy'), ('arrowy', 'ignorant'), ('ignorant', 'plantings'), ('plantings', 'forth'), ('forth', 'plautus'), ('plautus', 'improbability'), ('improbability', 'indestructible'), ('indestructible', 'sneezes'), ('sneezes', 'dead')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42335"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram= [a for a in ngrams(unigram, 2, pad_left=True,pad_right=True,left_pad_symbol='<s>', right_pad_symbol=\"</s>\")]\n",
    "print(bigram[:25])\n",
    "len(bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('madam', 1)]\n"
     ]
    }
   ],
   "source": [
    "cfreq_2gram = nltk.ConditionalFreqDist(bigram)\n",
    "cprob_2gram = nltk.ConditionalProbDist(cfreq_2gram, nltk.MLEProbDist)\n",
    "def bigram_prob_with_add1smoothing(word1, word2):\n",
    "    cprob_2gram_add1=float((((1+cfreq_2gram[word1][word2])/(len(cfreq_2gram)+sum(cfreq_2gram[word1].values())))))\n",
    "    return cprob_2gram_add1\n",
    "#print(cfreq_2gram.tabulate(conditions= ['khalifa', 'romance', 'tourist'], samples=range(100) ))\n",
    "print(cfreq_2gram['travel'].most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0002393074652438"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perplexity(2, test_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0002393311563496"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perplexity(2, train_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bigram_prob_with_gtsmoothing(word1, word2):\n",
    "    return SimpleGoodTuringProbDist(word1, word2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bigram_prob_with_add1smoothing(word1, word2):\n",
    "    cprob_2gram_add1=float((((1+cfreq_2gram[word1][word2])/(len(cfreq_2gram)+sum(cfreq_2gram[word1].values())))))\n",
    "    return cprob_2gram_add1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0002393074652438"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#bigram perplexity with Good-Turing smoothing( or add 1 smoothing )\n",
    "per_value_2gram= perplexity(2, test_corpus)\n",
    "per_value_2gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42336"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram= [a for a in ngrams(unigram, 3, pad_left= True, pad_right=True, left_pad_symbol= \"<s>\", right_pad_symbol=\"</s>\")]\n",
    "len(trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trigrams_as_bigrams=[]\n",
    "trigrams_as_bigrams.extend([((t[0],t[1]), t[2]) for t in trigram])\n",
    "cfreq_3gram = nltk.ConditionalFreqDist(trigrams_as_bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('<s>', '<s>'), 'gallowes'), (('<s>', 'gallowes'), 'therin'), (('gallowes', 'therin'), 'circumvented'), (('therin', 'circumvented'), 'leuell'), (('circumvented', 'leuell'), 'himmalehs'), (('leuell', 'himmalehs'), 'forsaking'), (('himmalehs', 'forsaking'), 'mitchell'), (('forsaking', 'mitchell'), 'civil'), (('mitchell', 'civil'), 'purchases'), (('civil', 'purchases'), 'marcus'), (('purchases', 'marcus'), 'purveyed'), (('marcus', 'purveyed'), 'maritime'), (('purveyed', 'maritime'), 'vpspring'), (('maritime', 'vpspring'), 'surging'), (('vpspring', 'surging'), 'palsied'), (('surging', 'palsied'), 'disinterested'), (('palsied', 'disinterested'), 'arrowy'), (('disinterested', 'arrowy'), 'ignorant'), (('arrowy', 'ignorant'), 'plantings'), (('ignorant', 'plantings'), 'forth'), (('plantings', 'forth'), 'plautus'), (('forth', 'plautus'), 'improbability'), (('plautus', 'improbability'), 'indestructible'), (('improbability', 'indestructible'), 'sneezes'), (('indestructible', 'sneezes'), 'dead'), (('sneezes', 'dead'), 'stickytoes'), (('dead', 'stickytoes'), 'mote'), (('stickytoes', 'mote'), 'transpired'), (('mote', 'transpired'), 'reclining'), (('transpired', 'reclining'), 'dictaean'), (('reclining', 'dictaean'), 'betrayal'), (('dictaean', 'betrayal'), 'border'), (('betrayal', 'border'), 'starry'), (('border', 'starry'), 'briskness'), (('starry', 'briskness'), 'opponent'), (('briskness', 'opponent'), 'usurper'), (('opponent', 'usurper'), 'litheness'), (('usurper', 'litheness'), 'markest'), (('litheness', 'markest'), 'dothan'), (('markest', 'dothan'), 'unmake')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FreqDist()"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(trigrams_as_bigrams[:40])\n",
    "cfreq_3gram[('lanchar', 'oshua')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'reclining': 1})"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfreq_2gram['transpired']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying to generate text from corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter, OrderedDict\n",
    "from operator import itemgetter\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-148-ec4507b7b20d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtest_unigram\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpunctuation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtest_unigram\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtest_unigram\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtest_bigram\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0ma\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mngrams\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_unigram\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_bigram\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test' is not defined"
     ]
    }
   ],
   "source": [
    "test_unigram = [w.strip(string.punctuation).lower() for w in test.words()]\n",
    "test_unigram= [word for word in test_unigram if word not in stopwords]\n",
    "\n",
    "test_bigram= [a for a in ngrams(test_unigram, 2)]\n",
    "len(test_bigram)\n",
    "\n",
    "test_trigram= [a for a in ngrams(test_unigram, 3)]\n",
    "len(test_trigram)\n",
    "\n",
    "test_frequency= nltk.FreqDist(test_bigram)\n",
    "test_sorted_freq= OrderedDict(reversed(sorted(test_frequency.items(), key= itemgetter(1))))\n",
    "\n",
    "test_cfreq_2gram= nltk.ConditionalFreqDist(test_bigram)\n",
    "\n",
    "test_cprob_2gram= nltk.ConditionalProbDist(test_cfreq_2gram, nltk.MLEProbDist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = Counter(tokens)\n",
    "total_count = len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 186091),\n",
       " ('the', 125748),\n",
       " ('and', 78846),\n",
       " ('.', 73746),\n",
       " ('of', 70078),\n",
       " (':', 47406),\n",
       " ('to', 46443),\n",
       " ('a', 32504),\n",
       " ('in', 31959),\n",
       " ('I', 30221),\n",
       " (';', 27329),\n",
       " ('that', 27289),\n",
       " ('he', 22198),\n",
       " ('his', 20585),\n",
       " (\"'\", 19873),\n",
       " ('it', 19734),\n",
       " ('was', 18558),\n",
       " ('for', 16860),\n",
       " ('not', 16834),\n",
       " ('with', 16827)]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts.most_common(n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for word in counts:\n",
    "    counts[word] /= float(total_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "throughout 19 him because Jesus in he she . , ? was : purpose well , dependence single which am said . . be any 2 thirty shalt had realm down , Moses nigh doors accursed , one shalt in her partial , , a 7 his 25 , , assured superior the delivered the for his a led as Nevertheless the , of drugged any to little in ; most then , did . the a the , of but betted , to and of was nor .\" a Perceiving door once known top 9 the I pieces astonishment\n"
     ]
    }
   ],
   "source": [
    "# generating 100 words at random of the language\n",
    "text=[]\n",
    "for _ in range(100):\n",
    "    r = random.random()\n",
    "    accumulator = .0\n",
    " \n",
    "    for word, freq in counts.items():\n",
    "        accumulator += freq\n",
    " \n",
    "        if accumulator >= r:\n",
    "            text.append(word)\n",
    "            break\n",
    "print(' '.join(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Random Text generate using bigram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_generate_bigram(cfd_bigram, cfd_trigram, word, length=10):\n",
    "    for i in range(length):\n",
    "        word_list = nltk.word_tokenize(word)\n",
    "        print(word, end=\" \")\n",
    "        if(len(word_list) == 1):\n",
    "            word= cfd_bigram[word].max()\n",
    "            #word= random.choice(list(cfreq_2gram[word].keys()))\n",
    "        elif(len(word_list) == 2):\n",
    "            word= cfd_trigram[(word_list[0], word_list[1])].max()\n",
    "        else:\n",
    "            print(\"Enter the entity no longer than 2 words!!!\")\n",
    "        #word= random.choice(list(cfreq_2gram[word].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cricket bruited anani disabling jot gaudiest shallows queene unanswered _engagement_ "
     ]
    }
   ],
   "source": [
    "text_generate_bigram(cfreq_2gram, cfreq_3gram, 'cricket')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "education eliud darwin mungo azekah celibate girt ezri consonancy transgressors "
     ]
    }
   ],
   "source": [
    "text_generate_bigram(cfreq_2gram,cfreq_3gram, \"education\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comedy adornments leprous verified presumptive feverish forsookest forge armed collectively "
     ]
    }
   ],
   "source": [
    "text_generate_bigram(cfreq_2gram,cfreq_3gram, \"comedy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rich mazy federal excursive verbally evildoers reparation unpent dst misdoubt "
     ]
    }
   ],
   "source": [
    "text_generate_bigram(cfreq_2gram,cfreq_3gram, \"rich\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "poor kirjath hatefullest trip vulcans translated hautboys crowns harass venome "
     ]
    }
   ],
   "source": [
    "text_generate_bigram(cfreq_2gram,cfreq_3gram, \"poor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS template based random text generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Virat', 'NNP'), ('Kohli', 'NNP'), ('is', 'VBZ'), ('a', 'DT'), ('great', 'JJ'), ('cricket', 'NN'), ('player', 'NN'), ('of', 'IN'), ('world', 'NN')]\n",
      "[('Education', 'NN'), ('gives', 'VBZ'), ('us', 'PRP'), ('a', 'DT'), ('knowledge', 'NN'), ('of', 'IN'), ('the', 'DT'), ('world', 'NN'), ('around', 'IN'), ('us', 'PRP')]\n",
      "[('Rich', 'NNP'), ('is', 'VBZ'), ('the', 'DT'), ('new', 'JJ'), ('trend', 'NN'), ('in', 'IN'), ('the', 'DT'), ('world', 'NN')]\n",
      "[('Comedy', 'NN'), ('is', 'VBZ'), ('a', 'DT'), ('light', 'JJ'), ('way', 'NN'), ('to', 'TO'), ('tell', 'VB'), ('the', 'DT'), ('truth', 'NN'), ('to', 'TO'), ('people', 'NNS'), ('.', '.')]\n",
      "[('biggest', 'JJS'), ('tragedy', 'NN'), ('here', 'RB'), ('is', 'VBZ'), ('that', 'IN'), ('the', 'DT'), ('accident', 'NN'), ('could', 'MD'), ('have', 'VB'), ('easily', 'RB'), ('been', 'VBN'), ('prevented', 'VBN')]\n"
     ]
    }
   ],
   "source": [
    "sent1= \"Virat Kohli is a great cricket player of world\"  #virat kohli\n",
    "sent2= \"Education gives us a knowledge of the world around us\"  #education\n",
    "sent3= \"Rich is the new trend in the world\"  #rich\n",
    "sent4= \"Comedy is a light way to tell the truth to people.\"  #comedy\n",
    "sent5= \"biggest tragedy here is that the accident could have easily been prevented\"   #tragedy\n",
    "\n",
    "\n",
    "for sentence in [sent1, sent2, sent3, sent4, sent5]:\n",
    "    words= nltk.word_tokenize(sentence)\n",
    "    print(nltk.pos_tag(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "template1= ['NN', 'VB' , 'JJ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
